# Tutorial Apache Airflow

> Guia completo para desenvolvedores backend aprenderem Apache Airflow, desde conceitos b√°sicos at√© a aplica√ß√£o no DestaquesGovBr.

## √çndice

1. [Introdu√ß√£o e Motiva√ß√£o](#introducao-e-motivacao)
2. [Arquitetura e Infraestrutura](#arquitetura-e-infraestrutura)
3. [Setup Local com Astro CLI](#setup-local-com-astro-cli)
4. [Conceitos Fundamentais](#conceitos-fundamentais)
5. [Operadores Essenciais](#operadores-essenciais)
6. [Conceitos Intermedi√°rios](#conceitos-intermediarios)
7. [Padr√µes Avan√ßados](#padroes-avancados)
8. [Boas Pr√°ticas](#boas-praticas)
9. [Monitoramento e Debug](#monitoramento-e-debug)
10. [Aplica√ß√£o no DestaquesGovBr](#aplicacao-no-destaquesgovbr)
11. [Exerc√≠cios Pr√°ticos](#exercicios-praticos)
12. [Gloss√°rio](#glossario)

---

## Introdu√ß√£o e Motiva√ß√£o

### O que √© Apache Airflow?

Apache Airflow √© uma plataforma open-source para **orquestra√ß√£o de workflows** e **pipelines de dados**. Criado pelo Airbnb em 2014 e doado √† Apache Software Foundation em 2016, tornou-se o padr√£o de mercado para engenharia de dados.

**Principais caracter√≠sticas:**
- **Workflows como c√≥digo** (Python)
- **Agendamento** flex√≠vel (cron-like)
- **Monitoramento** visual via interface web
- **Extensibilidade** atrav√©s de operadores e plugins
- **Depend√™ncias** declarativas entre tarefas

### Por que Airflow para Pipelines de Dados?

```mermaid
flowchart TD
    subgraph "Sem Orquestrador"
        A1[Cron Job 1] --> B1[???]
        A2[Cron Job 2] --> B2[???]
        A3[Cron Job 3] --> B3[???]
    end

    subgraph "Com Airflow"
        C[Scheduler] --> D[DAG]
        D --> E[Task 1]
        E --> F[Task 2]
        F --> G[Task 3]
        G --> H[‚úì Monitoramento]
    end
```

| Problema | Solu√ß√£o Airflow |
|----------|-----------------|
| Scripts cron sem visibilidade | Interface web com hist√≥rico completo |
| Falhas silenciosas | Alertas, retries autom√°ticos, logs centralizados |
| Depend√™ncias manuais | Depend√™ncias declarativas entre tasks |
| Dif√≠cil fazer backfill | Backfill nativo com controle de datas |
| C√≥digo espalhado | Workflows versionados como c√≥digo Python |

### Compara√ß√£o com Alternativas

| Ferramenta | Pr√≥s | Contras | Ideal para |
|------------|------|---------|------------|
| **Airflow** | Maduro, comunidade enorme, extens√≠vel, padr√£o de mercado | Curva de aprendizado inicial | Pipelines complexos, produ√ß√£o |
| **Prefect** | Moderno, Pythonic, cloud-native | Menos maduro, comunidade menor | Times pequenos, startups |
| **Dagster** | Type-safe, excelente DX, asset-centric | Comunidade menor, menos integra√ß√µes | Data mesh, ML pipelines |
| **Luigi** | Simples, leve | Menos features, desenvolvimento lento | Pipelines simples |
| **GitHub Actions** | J√° integrado ao CI/CD | N√£o foi feito para dados, sem backfill | CI/CD, n√£o pipelines de dados |

**Por que escolhemos Airflow?**

1. **Maturidade** - 10+ anos de uso em produ√ß√£o
2. **Comunidade** - 35k+ stars no GitHub, milhares de contribuidores
3. **Ecossistema** - 200+ operadores prontos (AWS, GCP, Databricks, etc.)
4. **Astronomer** - Empresa dedicada com ferramentas excelentes (Astro CLI)
5. **Empregabilidade** - Skill mais requisitada em data engineering

---

## Arquitetura e Infraestrutura

### Componentes Principais

```mermaid
flowchart TB
    subgraph "Airflow Architecture"
        UI[üåê Web Server<br/>Interface visual]
        SCH[‚è∞ Scheduler<br/>Agenda e dispara tasks]
        TRIG[‚ö° Triggerer<br/>Deferrable operators]
        EXEC[üîÑ Executor<br/>Distribui trabalho]
        WORK[üë∑ Workers<br/>Executam tasks]
        META[(üóÑÔ∏è Metadata DB<br/>PostgreSQL/MySQL)]
    end

    UI <--> META
    SCH <--> META
    SCH --> EXEC
    EXEC --> WORK
    TRIG <--> META
    WORK --> META

    style UI fill:#4CAF50
    style SCH fill:#2196F3
    style META fill:#FF9800
```

### Explica√ß√£o dos Componentes

| Componente | Fun√ß√£o | Tecnologia |
|------------|--------|------------|
| **Web Server** | Interface para visualiza√ß√£o, monitoramento e opera√ß√µes manuais | Flask/Gunicorn |
| **Scheduler** | L√™ DAGs, agenda execu√ß√µes, cria task instances | Processo Python |
| **Executor** | Define como/onde as tasks ser√£o executadas | LocalExecutor, CeleryExecutor, KubernetesExecutor |
| **Workers** | Processos que realmente executam o c√≥digo das tasks | Celery workers ou pods K8s |
| **Triggerer** | Gerencia deferrable operators (async) - economiza recursos | Processo Python (Airflow 2.2+) |
| **Metadata DB** | Armazena estado de DAGs, tasks, vari√°veis, conex√µes | PostgreSQL (recomendado) ou MySQL |

### Modos de Execu√ß√£o (Executors)

| Executor | Descri√ß√£o | Quando Usar |
|----------|-----------|-------------|
| **SequentialExecutor** | Uma task por vez, mesmo processo | Apenas testes/debug |
| **LocalExecutor** | M√∫ltiplas tasks em paralelo, mesmo servidor | Desenvolvimento, ambientes pequenos |
| **CeleryExecutor** | Workers distribu√≠dos via Celery + Redis/RabbitMQ | Produ√ß√£o com escala horizontal |
| **KubernetesExecutor** | Cada task em um pod isolado | Produ√ß√£o em K8s, isolamento m√°ximo |

> **Para o DestaquesGovBr**: Usaremos **LocalExecutor** no ambiente local (Astro CLI) e **KubernetesExecutor** em produ√ß√£o (Astronomer Cloud ou GKE).

---

## Setup Local com Astro CLI

### O que √© Astro CLI?

Astro CLI √© a ferramenta oficial da [Astronomer](https://www.astronomer.io/) para desenvolvimento local com Airflow. Ela abstrai toda a complexidade de configurar Docker, bancos de dados e o Airflow em si.

### Pr√©-requisitos

| Ferramenta | Vers√£o | Verifica√ß√£o |
|------------|--------|-------------|
| Docker | 24+ | `docker --version` |
| Docker Compose | 2.20+ | `docker compose version` |

### Instala√ß√£o do Astro CLI

=== "macOS (Homebrew)"
    ```bash
    brew install astro
    ```

=== "Linux"
    ```bash
    curl -sSL install.astronomer.io | sudo bash -s
    ```

=== "Windows (WSL2)"
    ```bash
    curl -sSL install.astronomer.io | sudo bash -s
    ```

**Verificar instala√ß√£o:**
```bash
astro version
# Astro CLI Version: 1.x.x
```

### Criando seu Primeiro Projeto

```bash
# Criar diret√≥rio do projeto
mkdir meu-projeto-airflow
cd meu-projeto-airflow

# Inicializar projeto Airflow
astro dev init
```

### Estrutura do Projeto

```
meu-projeto-airflow/
‚îú‚îÄ‚îÄ dags/                    # üìÇ Suas DAGs v√£o aqui
‚îÇ   ‚îî‚îÄ‚îÄ example_dag.py       #    DAG de exemplo
‚îú‚îÄ‚îÄ include/                 # üìÇ Arquivos auxiliares (SQL, configs)
‚îú‚îÄ‚îÄ plugins/                 # üìÇ Plugins customizados
‚îú‚îÄ‚îÄ tests/                   # üìÇ Testes das DAGs
‚îÇ   ‚îî‚îÄ‚îÄ dags/
‚îÇ       ‚îî‚îÄ‚îÄ test_dag_integrity.py
‚îú‚îÄ‚îÄ .astro/                  # üìÇ Configura√ß√µes Astro (n√£o editar)
‚îú‚îÄ‚îÄ .dockerignore
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ Dockerfile               # üê≥ Imagem customizada
‚îú‚îÄ‚îÄ packages.txt             # üì¶ Depend√™ncias de sistema (apt)
‚îú‚îÄ‚îÄ requirements.txt         # üì¶ Depend√™ncias Python (pip)
‚îî‚îÄ‚îÄ airflow_settings.yaml    # ‚öôÔ∏è Variables, Connections, Pools
```

### Comandos Principais

```bash
# Iniciar ambiente local (primeira vez demora ~2min)
astro dev start

# Acessar a UI: http://localhost:8080
# Login: admin / admin

# Parar ambiente
astro dev stop

# Reiniciar (ap√≥s mudar requirements.txt ou Dockerfile)
astro dev restart

# Ver logs em tempo real
astro dev logs -f

# Entrar no container do scheduler (debug)
astro dev bash

# Executar comando dentro do container
astro dev run airflow dags list

# Rodar testes
astro dev pytest
```

### Adicionando Depend√™ncias

**Python (requirements.txt):**
```txt
pandas==2.1.0
requests==2.31.0
beautifulsoup4==4.12.0
```

**Sistema (packages.txt):**
```txt
libpq-dev
gcc
```

Ap√≥s modificar, reinicie:
```bash
astro dev restart
```

---

## Conceitos Fundamentais

### DAG (Directed Acyclic Graph)

Uma DAG √© o conceito central do Airflow - um grafo direcionado ac√≠clico que define:
- **Quais tasks** executar
- **Em que ordem** (depend√™ncias)
- **Quando** executar (schedule)

```mermaid
flowchart LR
    subgraph "DAG: processo_diario"
        A[extrair_dados] --> B[transformar_dados]
        B --> C[carregar_dados]
        B --> D[gerar_relatorio]
        C --> E[notificar_sucesso]
        D --> E
    end
```

### Anatomia de uma DAG

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

# Argumentos padr√£o para todas as tasks da DAG
default_args = {
    'owner': 'data-team',
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'email_on_failure': True,
    'email': ['alerts@destaquesgovbr.com'],
}

# Defini√ß√£o da DAG
with DAG(
    dag_id='minha_primeira_dag',           # Identificador √∫nico
    description='Minha primeira DAG',       # Descri√ß√£o
    start_date=datetime(2024, 1, 1),        # Data de in√≠cio
    schedule='@daily',                      # Agendamento (cron ou preset)
    catchup=False,                          # N√£o executar datas passadas
    tags=['tutorial', 'exemplo'],           # Tags para organiza√ß√£o
    default_args=default_args,
) as dag:

    def extrair():
        print("Extraindo dados...")
        return {"registros": 100}

    def transformar(**context):
        # Acessar dados da task anterior via XCom
        dados = context['ti'].xcom_pull(task_ids='extrair')
        print(f"Transformando {dados['registros']} registros")

    # Defini√ß√£o das tasks
    task_extrair = PythonOperator(
        task_id='extrair',
        python_callable=extrair,
    )

    task_transformar = PythonOperator(
        task_id='transformar',
        python_callable=transformar,
    )

    # Definir depend√™ncias
    task_extrair >> task_transformar  # extrair roda antes de transformar
```

### Presets de Schedule

| Preset | Equivalente Cron | Descri√ß√£o |
|--------|------------------|-----------|
| `@once` | - | Executa apenas uma vez |
| `@hourly` | `0 * * * *` | Todo in√≠cio de hora |
| `@daily` | `0 0 * * *` | Todo dia √† meia-noite |
| `@weekly` | `0 0 * * 0` | Todo domingo √† meia-noite |
| `@monthly` | `0 0 1 * *` | Todo dia 1 √† meia-noite |
| `@yearly` | `0 0 1 1 *` | Todo 1 de janeiro |
| `None` | - | Apenas trigger manual |

### Depend√™ncias entre Tasks

```python
# Usando operadores >> e <<
task_a >> task_b >> task_c  # A -> B -> C

# Equivalente com set_downstream/set_upstream
task_a.set_downstream(task_b)
task_b.set_downstream(task_c)

# Depend√™ncias paralelas
task_a >> [task_b, task_c] >> task_d
#     A
#    / \
#   B   C
#    \ /
#     D

# Depend√™ncias cruzadas
[task_a, task_b] >> [task_c, task_d]
# A e B devem terminar antes de C e D come√ßarem
```

### Ciclo de Vida de uma Task

```mermaid
stateDiagram-v2
    [*] --> none: DAG carregada
    none --> scheduled: Scheduler detecta
    scheduled --> queued: Slot dispon√≠vel
    queued --> running: Worker pega
    running --> success: Sucesso
    running --> failed: Falha
    failed --> up_for_retry: Tem retries
    up_for_retry --> scheduled: Aguarda retry_delay
    failed --> [*]: Sem retries
    success --> [*]
```

---

## Operadores Essenciais

### PythonOperator

Executa uma fun√ß√£o Python.

```python
from airflow.operators.python import PythonOperator

def processar_dados(nome, **kwargs):
    """
    **kwargs recebe o contexto do Airflow automaticamente:
    - ti (TaskInstance)
    - ds (execution_date como string YYYY-MM-DD)
    - execution_date, etc.
    """
    print(f"Processando dados para {nome}")
    print(f"Data de execu√ß√£o: {kwargs['ds']}")
    return "resultado"  # Automaticamente salvo como XCom

task = PythonOperator(
    task_id='processar',
    python_callable=processar_dados,
    op_kwargs={'nome': 'DestaquesGovBr'},  # Argumentos extras
)
```

### BashOperator

Executa comandos shell.

```python
from airflow.operators.bash import BashOperator

# Comando simples
task_listar = BashOperator(
    task_id='listar_arquivos',
    bash_command='ls -la /tmp',
)

# Com templates Jinja
task_baixar = BashOperator(
    task_id='baixar_dados',
    bash_command='curl -o /tmp/dados_{{ ds }}.json https://api.exemplo.com/dados',
)

# Script externo
task_script = BashOperator(
    task_id='executar_script',
    bash_command='/opt/scripts/processar.sh {{ ds }} ',  # Espa√ßo final importante!
)
```

### EmptyOperator

Placeholder para organiza√ß√£o visual ou checkpoints.

```python
from airflow.operators.empty import EmptyOperator

inicio = EmptyOperator(task_id='inicio')
fim = EmptyOperator(task_id='fim')

inicio >> [task_a, task_b, task_c] >> fim
```

### Sensors

Aguardam uma condi√ß√£o ser satisfeita antes de prosseguir.

```python
from airflow.sensors.filesystem import FileSensor
from airflow.providers.http.sensors.http import HttpSensor

# Aguardar arquivo existir
sensor_arquivo = FileSensor(
    task_id='aguardar_arquivo',
    filepath='/data/input/dados.csv',
    poke_interval=60,      # Verificar a cada 60s
    timeout=3600,          # Timeout ap√≥s 1 hora
    mode='poke',           # 'poke' (s√≠ncrono) ou 'reschedule' (libera slot)
)

# Aguardar API responder
sensor_api = HttpSensor(
    task_id='aguardar_api',
    http_conn_id='api_externa',
    endpoint='/health',
    response_check=lambda response: response.status_code == 200,
    poke_interval=30,
    timeout=600,
)
```

### Tabela de Operadores Comuns

| Operador | Uso | Provider |
|----------|-----|----------|
| `PythonOperator` | Fun√ß√µes Python | Core |
| `BashOperator` | Comandos shell | Core |
| `EmptyOperator` | Placeholder | Core |
| `HttpOperator` | Chamadas HTTP | `apache-airflow-providers-http` |
| `PostgresOperator` | Queries SQL | `apache-airflow-providers-postgres` |
| `S3ToGCSOperator` | Transfer√™ncia S3‚ÜíGCS | `apache-airflow-providers-google` |
| `DockerOperator` | Containers Docker | `apache-airflow-providers-docker` |
| `KubernetesPodOperator` | Pods K8s | `apache-airflow-providers-cncf-kubernetes` |

---

## Conceitos Intermedi√°rios

### execution_date e Data Intervals

O conceito mais importante (e confuso) do Airflow!

```mermaid
gantt
    title Linha do Tempo - DAG @daily
    dateFormat YYYY-MM-DD HH:mm
    axisFormat %d/%m %H:%M

    section Data Interval
    [2024-01-01 00:00 - 2024-01-02 00:00] :a1, 2024-01-01 00:00, 24h
    [2024-01-02 00:00 - 2024-01-03 00:00] :a2, 2024-01-02 00:00, 24h

    section Execu√ß√£o Real
    Run 1 (processa dados de 01/01) :milestone, m1, 2024-01-02 00:00, 0d
    Run 2 (processa dados de 02/01) :milestone, m2, 2024-01-03 00:00, 0d
```

**Conceitos-chave:**

| Termo | Significado |
|-------|-------------|
| `data_interval_start` | In√≠cio do per√≠odo de dados sendo processado |
| `data_interval_end` | Fim do per√≠odo de dados |
| `logical_date` (antes: `execution_date`) | Igual a `data_interval_start` |
| **Execu√ß√£o real** | Acontece AP√ìS o `data_interval_end` |

**Exemplo pr√°tico:**
- DAG `@daily` com `start_date=2024-01-01`
- Dia 02/01 √†s 00:00: Airflow executa processando dados de 01/01
- `logical_date = 2024-01-01`
- `data_interval_start = 2024-01-01 00:00`
- `data_interval_end = 2024-01-02 00:00`

### Acessando Datas no C√≥digo

```python
from airflow.decorators import task

@task
def processar(**context):
    # Novas vari√°veis (Airflow 2.2+)
    data_interval_start = context['data_interval_start']  # pendulum datetime
    data_interval_end = context['data_interval_end']
    logical_date = context['logical_date']

    # Templates Jinja (strings)
    ds = context['ds']                  # "2024-01-01"
    ds_nodash = context['ds_nodash']    # "20240101"

    print(f"Processando dados de {data_interval_start} at√© {data_interval_end}")
```

**Templates Jinja em strings:**
```python
BashOperator(
    task_id='baixar',
    bash_command='wget https://api.com/dados?date={{ ds }}',
)
```

### Catchup e Backfill

**Catchup** (na DAG):
```python
with DAG(
    dag_id='minha_dag',
    start_date=datetime(2024, 1, 1),
    schedule='@daily',
    catchup=True,  # Default: True - executa todas as datas desde start_date
) as dag:
    ...
```

- `catchup=True`: Ao criar/ativar a DAG, executa todas as datas pendentes
- `catchup=False`: S√≥ executa a partir de agora

**Backfill** (via CLI):
```bash
# Executar para um range de datas espec√≠fico
astro dev run airflow dags backfill \
    -s 2024-01-01 \
    -e 2024-01-31 \
    minha_dag

# S√≥ mostrar o que seria executado (dry-run)
astro dev run airflow dags backfill \
    -s 2024-01-01 \
    -e 2024-01-31 \
    --dry-run \
    minha_dag
```

### XComs (Cross-Communication)

Comunica√ß√£o entre tasks dentro de uma DAG.

```python
from airflow.decorators import task, dag
from datetime import datetime

@dag(schedule='@daily', start_date=datetime(2024, 1, 1), catchup=False)
def pipeline_xcoms():

    @task
    def extrair():
        dados = [1, 2, 3, 4, 5]
        return dados  # Automaticamente salvo como XCom

    @task
    def transformar(dados):  # Recebe automaticamente com TaskFlow
        return [x * 2 for x in dados]

    @task
    def carregar(dados):
        print(f"Carregando: {dados}")

    # Encadeamento autom√°tico via TaskFlow
    dados_extraidos = extrair()
    dados_transformados = transformar(dados_extraidos)
    carregar(dados_transformados)

pipeline_xcoms()
```

**XCom tradicional (sem TaskFlow):**
```python
def extrair(**context):
    dados = [1, 2, 3]
    context['ti'].xcom_push(key='meus_dados', value=dados)

def transformar(**context):
    dados = context['ti'].xcom_pull(task_ids='extrair', key='meus_dados')
    # ou usar key='return_value' para o retorno da fun√ß√£o
```

> ‚ö†Ô∏è **Aten√ß√£o**: XComs s√£o armazenados no banco de dados. N√£o use para dados grandes (>48KB)! Para dados grandes, use arquivos tempor√°rios ou object storage.

### Variables e Connections

**Variables** - Configura√ß√µes globais:
```python
from airflow.models import Variable

# Definir via UI: Admin > Variables
# Ou via c√≥digo:
api_key = Variable.get('COGFY_API_KEY')
config = Variable.get('scraper_config', deserialize_json=True)
```

**Connections** - Credenciais de sistemas externos:
```python
from airflow.hooks.base import BaseHook

# Definir via UI: Admin > Connections
# Ou via airflow_settings.yaml no Astro

conn = BaseHook.get_connection('postgres_destaquesgovbr')
print(conn.host, conn.port, conn.login)
```

**airflow_settings.yaml (Astro):**
```yaml
airflow:
  variables:
    - variable_name: ENVIRONMENT
      variable_value: development

  connections:
    - conn_id: postgres_destaquesgovbr
      conn_type: postgres
      conn_host: localhost
      conn_port: 5432
      conn_login: airflow
      conn_password: airflow
      conn_schema: destaquesgovbr
```

### TaskFlow API (@task decorator)

Forma moderna e mais Pythonic de escrever DAGs (Airflow 2.0+):

```python
from airflow.decorators import dag, task
from datetime import datetime

@dag(
    schedule='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['taskflow', 'exemplo'],
)
def etl_taskflow():
    """DAG usando TaskFlow API - mais limpa e Pythonic."""

    @task
    def extract():
        """Extrai dados da fonte."""
        return {'usuarios': [1, 2, 3], 'total': 3}

    @task
    def transform(dados: dict):
        """Transforma os dados."""
        return {
            'usuarios_processados': [u * 10 for u in dados['usuarios']],
            'total': dados['total'],
        }

    @task
    def load(dados: dict):
        """Carrega no destino."""
        print(f"Carregando {dados['total']} usu√°rios: {dados['usuarios_processados']}")

    # Encadeamento natural
    dados_brutos = extract()
    dados_transformados = transform(dados_brutos)
    load(dados_transformados)

# Instanciar a DAG
etl_taskflow()
```

---

## Padr√µes Avan√ßados

### Branching (Execu√ß√£o Condicional)

```python
from airflow.decorators import dag, task
from airflow.operators.empty import EmptyOperator
from datetime import datetime
import random

@dag(schedule='@daily', start_date=datetime(2024, 1, 1), catchup=False)
def dag_branching():

    @task.branch
    def escolher_caminho():
        """Retorna o task_id da pr√≥xima task a executar."""
        valor = random.random()
        if valor > 0.5:
            return 'caminho_a'
        else:
            return 'caminho_b'

    @task
    def caminho_a():
        print("Executando caminho A")

    @task
    def caminho_b():
        print("Executando caminho B")

    @task(trigger_rule='none_failed_min_one_success')
    def finalizar():
        """Executa se pelo menos um caminho anterior teve sucesso."""
        print("Finalizando...")

    # Fluxo
    decisao = escolher_caminho()
    task_a = caminho_a()
    task_b = caminho_b()
    fim = finalizar()

    decisao >> [task_a, task_b] >> fim

dag_branching()
```

```mermaid
flowchart TD
    A[escolher_caminho] -->|"valor > 0.5"| B[caminho_a]
    A -->|"valor <= 0.5"| C[caminho_b]
    B --> D[finalizar]
    C --> D
```

### Dynamic Task Mapping

Criar tasks dinamicamente baseado em dados (Airflow 2.3+):

```python
from airflow.decorators import dag, task
from datetime import datetime

@dag(schedule='@daily', start_date=datetime(2024, 1, 1), catchup=False)
def dag_dynamic_tasks():

    @task
    def get_lista_orgaos():
        """Retorna lista de √≥rg√£os para processar."""
        return [
            {'id': 1, 'nome': 'MEC'},
            {'id': 2, 'nome': 'MS'},
            {'id': 3, 'nome': 'MF'},
        ]

    @task
    def processar_orgao(orgao: dict):
        """Processa um √≥rg√£o espec√≠fico."""
        print(f"Processando {orgao['nome']} (ID: {orgao['id']})")
        return f"Processado: {orgao['nome']}"

    @task
    def consolidar(resultados: list):
        """Consolida todos os resultados."""
        print(f"Consolidando {len(resultados)} resultados")
        for r in resultados:
            print(f"  - {r}")

    # Dynamic mapping - cria N tasks automaticamente
    orgaos = get_lista_orgaos()
    resultados = processar_orgao.expand(orgao=orgaos)
    consolidar(resultados)

dag_dynamic_tasks()
```

### Task Groups

Organizar tasks visualmente:

```python
from airflow.decorators import dag, task, task_group
from datetime import datetime

@dag(schedule='@daily', start_date=datetime(2024, 1, 1), catchup=False)
def dag_task_groups():

    @task
    def inicio():
        print("Iniciando pipeline")

    @task_group(group_id='etl_govbr')
    def etl_govbr():
        @task
        def extract_govbr():
            return ['noticia1', 'noticia2']

        @task
        def transform_govbr(dados):
            return [d.upper() for d in dados]

        @task
        def load_govbr(dados):
            print(f"Carregando gov.br: {dados}")

        dados = extract_govbr()
        transformados = transform_govbr(dados)
        load_govbr(transformados)

    @task_group(group_id='etl_ebc')
    def etl_ebc():
        @task
        def extract_ebc():
            return ['ebc1', 'ebc2']

        @task
        def load_ebc(dados):
            print(f"Carregando EBC: {dados}")

        dados = extract_ebc()
        load_ebc(dados)

    @task
    def fim():
        print("Pipeline finalizado")

    inicio() >> [etl_govbr(), etl_ebc()] >> fim()

dag_task_groups()
```

### Trigger Rules

Controle quando uma task deve executar:

| Trigger Rule | Descri√ß√£o |
|--------------|-----------|
| `all_success` (default) | Todas as tasks anteriores com sucesso |
| `all_failed` | Todas as tasks anteriores falharam |
| `all_done` | Todas terminaram (sucesso ou falha) |
| `one_success` | Pelo menos uma com sucesso |
| `one_failed` | Pelo menos uma falhou |
| `none_failed` | Nenhuma falhou (sucesso ou skipped) |
| `none_failed_min_one_success` | Nenhuma falhou E pelo menos uma com sucesso |
| `none_skipped` | Nenhuma foi skipped |
| `always` | Sempre executa |

```python
@task(trigger_rule='none_failed_min_one_success')
def task_de_limpeza():
    """Executa mesmo se algumas tasks anteriores foram skipped."""
    pass
```

---

## Boas Pr√°ticas

### ‚úÖ Fazer

| Pr√°tica | Por qu√™ |
|---------|---------|
| **DAGs idempotentes** | Executar m√∫ltiplas vezes com mesma data = mesmo resultado |
| **Tasks at√¥micas** | Cada task faz uma coisa bem |
| **Usar Connections** | Nunca hardcode credenciais no c√≥digo |
| **Testar DAGs localmente** | `astro dev pytest` antes de deploy |
| **Versionar no Git** | DAGs s√£o c√≥digo, trate como tal |
| **Usar TaskFlow API** | C√≥digo mais limpo e Pythonic |
| **Nomear tasks claramente** | `extrair_noticias_govbr` > `task1` |
| **Documentar DAGs** | Docstrings aparecem na UI |

### ‚ùå Evitar

| Anti-pattern | Problema | Solu√ß√£o |
|--------------|----------|---------|
| **C√≥digo no top-level** | Executa a cada parse do scheduler | Mover para dentro de fun√ß√µes |
| **XComs grandes** | Sobrecarrega banco de dados | Usar arquivos/object storage |
| **Imports pesados no top-level** | Lentid√£o no parse | Import dentro das fun√ß√µes |
| **Depend√™ncias circulares** | DAG inv√°lida | Redesenhar o fluxo |
| **Uma DAG gigante** | Dif√≠cil debug e manuten√ß√£o | Quebrar em m√∫ltiplas DAGs |
| **Hardcode de datas** | Quebra backfill | Usar `{{ ds }}` templates |
| **Ignorar retries** | Falhas transientes param pipeline | Configurar retries adequados |

### Exemplo de DAG Bem Estruturada

```python
"""
DAG: scraper_noticias_govbr

Extrai not√≠cias de gov.br diariamente.
Owner: data-team
"""
from airflow.decorators import dag, task
from airflow.models import Variable
from datetime import datetime, timedelta

# Configura√ß√µes via Variables (n√£o hardcoded)
DEFAULT_ARGS = {
    'owner': 'data-team',
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'email_on_failure': True,
}

@dag(
    dag_id='scraper_noticias_govbr',
    description='Extrai not√≠cias de gov.br diariamente',
    schedule='0 4 * * *',  # 4AM UTC
    start_date=datetime(2024, 1, 1),
    catchup=False,
    default_args=DEFAULT_ARGS,
    tags=['scraper', 'govbr', 'producao'],
    doc_md=__doc__,
)
def scraper_noticias_govbr():

    @task
    def extrair(data_interval_start=None, data_interval_end=None):
        # Import dentro da fun√ß√£o - n√£o sobrecarrega scheduler
        from meu_scraper import WebScraper

        scraper = WebScraper()
        return scraper.extrair(
            data_inicio=data_interval_start,
            data_fim=data_interval_end,
        )

    @task
    def validar(dados: list):
        if not dados:
            raise ValueError("Nenhum dado extra√≠do!")
        return dados

    @task
    def carregar(dados: list):
        from meu_loader import Loader

        loader = Loader(conn_id='postgres_destaquesgovbr')
        loader.carregar(dados)

    # Pipeline
    dados = extrair()
    dados_validados = validar(dados)
    carregar(dados_validados)

scraper_noticias_govbr()
```

---

## Monitoramento e Debug

### Interface Web do Airflow

Acesse em `http://localhost:8080` (Astro local).

#### Views Principais

| View | O que mostra | Quando usar |
|------|--------------|-------------|
| **Grid** | Timeline de execu√ß√µes por task | Vis√£o geral de sucesso/falha |
| **Graph** | Grafo de depend√™ncias | Entender estrutura da DAG |
| **Calendar** | Hist√≥rico mensal | Identificar padr√µes de falha |
| **Gantt** | Timeline de dura√ß√£o | Identificar gargalos |
| **Code** | C√≥digo-fonte da DAG | Debug r√°pido |

#### A√ß√µes Comuns na UI

- **Trigger DAG**: Executar manualmente
- **Clear**: Limpar estado de tasks para re-executar
- **Mark Success/Failed**: For√ßar estado
- **View Logs**: Ver output das tasks

### Logs

```bash
# Via Astro CLI
astro dev logs -f                    # Todos os logs
astro dev logs -f scheduler          # S√≥ scheduler
astro dev logs -f webserver          # S√≥ webserver

# Dentro do container
astro dev bash
cat ~/airflow/logs/dag_id=minha_dag/run_id=manual__2024-01-01/task_id=extrair/attempt=1.log
```

### Debug de Tasks

```bash
# Testar task isoladamente
astro dev run airflow tasks test minha_dag extrair 2024-01-01

# Listar DAGs
astro dev run airflow dags list

# Ver pr√≥ximas execu√ß√µes
astro dev run airflow dags next-execution minha_dag
```

### Callbacks para Alertas

```python
def alerta_falha(context):
    """Chamado quando task falha."""
    task_instance = context['task_instance']
    dag_id = context['dag'].dag_id

    # Enviar para Slack, email, etc.
    print(f"ALERTA: {dag_id}.{task_instance.task_id} falhou!")

def alerta_sucesso(context):
    """Chamado quando task tem sucesso."""
    print("Task executada com sucesso!")

@task(on_failure_callback=alerta_falha, on_success_callback=alerta_sucesso)
def task_com_alertas():
    pass
```

---

## Aplica√ß√£o no DestaquesGovBr

### Contexto Atual

Atualmente o pipeline de scraping do DestaquesGovBr √© orquestrado via **GitHub Actions**:

```mermaid
flowchart LR
    subgraph "GitHub Actions (atual)"
        A[4AM UTC Trigger] --> B[Job: scraper]
        B --> C[Job: ebc-scraper]
        C --> D[Job: upload-to-cogfy]
        D --> E[Job: wait-cogfy]
        E --> F[Job: enrich-themes]
    end
```

**Limita√ß√µes do modelo atual:**
- Sem interface visual para monitoramento
- Backfill manual e complexo
- Logs espalhados entre runs
- Sem retry autom√°tico granular por step
- Dif√≠cil processar datas espec√≠ficas

### Por que Migrar para Airflow?

| Aspecto | GitHub Actions | Airflow |
|---------|----------------|---------|
| **Backfill** | Manual, complexo | Nativo, um comando |
| **Monitoramento** | Logs por run | Dashboard visual |
| **Retries** | Por job inteiro | Por task granular |
| **Depend√™ncias** | Sequenciais simples | Grafo complexo |
| **Paralelismo** | Limitado | Dynamic mapping |
| **Hist√≥rico** | 90 dias | Ilimitado |

### Arquitetura Proposta

```mermaid
flowchart TB
    subgraph "Airflow Scheduler"
        SCHED[Trigger 4AM UTC]
    end

    subgraph "DAG: destaques_govbr_pipeline"
        SCHED --> SCRAPE_GOVBR[scrape_govbr<br/>160+ sites]
        SCHED --> SCRAPE_EBC[scrape_ebc<br/>Fontes EBC]

        SCRAPE_GOVBR --> UPLOAD[upload_to_cogfy]
        SCRAPE_EBC --> UPLOAD

        UPLOAD --> WAIT[wait_cogfy<br/>Sensor ~20min]
        WAIT --> ENRICH[enrich_themes]
        ENRICH --> INDEX[index_typesense]
        INDEX --> NOTIFY[notify_success]
    end

    subgraph "Servi√ßos Externos"
        COGFY[(Cogfy LLM)]
        HF[(HuggingFace)]
        TS[(Typesense)]
    end

    UPLOAD --> COGFY
    WAIT --> COGFY
    ENRICH --> HF
    INDEX --> TS

    style SCRAPE_GOVBR fill:#4CAF50
    style SCRAPE_EBC fill:#4CAF50
    style WAIT fill:#FF9800
```

### DAG de Exemplo: Orquestra√ß√£o do Scraper

```python
"""
DAG: destaques_govbr_pipeline

Pipeline completo de scraping e enriquecimento de not√≠cias governamentais.
Executa diariamente √†s 4AM UTC.

Owner: data-team
Documenta√ß√£o: https://destaquesgovbr.github.io/docs/workflows/scraper-pipeline/
"""
from airflow.decorators import dag, task
from airflow.models import Variable
from airflow.providers.http.sensors.http import HttpSensor
from datetime import datetime, timedelta
import logging

logger = logging.getLogger(__name__)

DEFAULT_ARGS = {
    'owner': 'data-team',
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'email_on_failure': True,
    'email': ['alerts@destaquesgovbr.com'],
}

@dag(
    dag_id='destaques_govbr_pipeline',
    description='Pipeline de scraping e enriquecimento de not√≠cias gov.br',
    schedule='0 4 * * *',  # 4AM UTC diariamente
    start_date=datetime(2024, 1, 1),
    catchup=False,
    default_args=DEFAULT_ARGS,
    tags=['scraper', 'destaques-govbr', 'producao'],
    doc_md=__doc__,
    max_active_runs=1,  # Apenas uma execu√ß√£o por vez
)
def destaques_govbr_pipeline():

    @task
    def scrape_govbr(data_interval_start=None, data_interval_end=None):
        """
        Executa scraping de ~160 sites gov.br.

        Usa WebScraper do reposit√≥rio scraper.
        Salva resultados no HuggingFace Dataset.
        """
        # Import dentro da fun√ß√£o para n√£o sobrecarregar scheduler
        from govbrnews_scraper import WebScraper, DatasetManager

        logger.info(f"Iniciando scrape gov.br: {data_interval_start} - {data_interval_end}")

        scraper = WebScraper()
        noticias = scraper.run()

        dataset_manager = DatasetManager()
        dataset_manager.upload(noticias)

        logger.info(f"Scrape gov.br conclu√≠do: {len(noticias)} not√≠cias")
        return {'fonte': 'govbr', 'total': len(noticias)}

    @task
    def scrape_ebc(data_interval_start=None, data_interval_end=None):
        """
        Executa scraping de fontes EBC (Ag√™ncia Brasil, etc).
        """
        from govbrnews_scraper import EBCWebScraper, DatasetManager

        logger.info(f"Iniciando scrape EBC: {data_interval_start} - {data_interval_end}")

        scraper = EBCWebScraper()
        noticias = scraper.run()

        dataset_manager = DatasetManager()
        dataset_manager.upload(noticias)

        logger.info(f"Scrape EBC conclu√≠do: {len(noticias)} not√≠cias")
        return {'fonte': 'ebc', 'total': len(noticias)}

    @task
    def upload_to_cogfy(resultados_scraping: list):
        """
        Faz upload das not√≠cias para Cogfy para classifica√ß√£o via LLM.
        """
        from govbrnews_scraper import CogfyManager

        total = sum(r['total'] for r in resultados_scraping)
        logger.info(f"Enviando {total} not√≠cias para Cogfy")

        cogfy = CogfyManager()
        batch_id = cogfy.upload_for_processing()

        logger.info(f"Upload Cogfy conclu√≠do. Batch ID: {batch_id}")
        return batch_id

    @task.sensor(
        poke_interval=60,  # Verificar a cada 60s
        timeout=2400,      # Timeout 40min (processamento ~20min + margem)
        mode='reschedule', # Libera slot enquanto aguarda
    )
    def wait_cogfy_processing(batch_id: str):
        """
        Aguarda processamento do Cogfy (~20 minutos).
        Usa Sensor para n√£o bloquear workers.
        """
        from govbrnews_scraper import CogfyManager

        cogfy = CogfyManager()
        status = cogfy.check_batch_status(batch_id)

        logger.info(f"Status Cogfy: {status}")
        return status == 'completed'

    @task
    def enrich_themes():
        """
        Enriquece not√≠cias com temas da √°rvore tem√°tica e sum√°rios.
        """
        from govbrnews_scraper import EnrichmentManager

        logger.info("Iniciando enriquecimento de temas")

        enricher = EnrichmentManager()
        resultados = enricher.run()

        logger.info(f"Enriquecimento conclu√≠do: {resultados['processados']} not√≠cias")
        return resultados

    @task
    def index_typesense(resultados_enriquecimento: dict):
        """
        Indexa not√≠cias enriquecidas no Typesense.
        """
        from govbrnews_scraper import TypesenseIndexer

        logger.info("Iniciando indexa√ß√£o Typesense")

        indexer = TypesenseIndexer()
        indexer.run()

        logger.info("Indexa√ß√£o Typesense conclu√≠da")

    @task
    def notify_success():
        """
        Notifica sucesso do pipeline via Slack/Email.
        """
        logger.info("Pipeline executado com sucesso!")
        # TODO: Integrar com Slack/Email

    # === DEFINI√á√ÉO DO FLUXO ===

    # 1. Scraping em paralelo
    govbr_result = scrape_govbr()
    ebc_result = scrape_ebc()

    # 2. Upload para Cogfy (ap√≥s ambos scrapers)
    batch_id = upload_to_cogfy([govbr_result, ebc_result])

    # 3. Aguardar processamento Cogfy
    cogfy_done = wait_cogfy_processing(batch_id)

    # 4. Enriquecimento (ap√≥s Cogfy)
    enrichment_result = enrich_themes()

    # 5. Indexa√ß√£o
    index_typesense(enrichment_result)

    # 6. Notifica√ß√£o
    notify_success()

    # Depend√™ncias expl√≠citas adicionais
    cogfy_done >> enrichment_result

# Instanciar a DAG
destaques_govbr_pipeline()
```

### Pr√≥ximos Passos para Ado√ß√£o

1. **Local**: Configurar Astro CLI no reposit√≥rio scraper
2. **Adaptar**: Refatorar scripts atuais para formato task
3. **Testar**: Validar pipeline localmente com dados de teste
4. **Deploy**: Configurar Astronomer Cloud ou self-hosted no GKE
5. **Migrar**: Transi√ß√£o gradual de GitHub Actions para Airflow

---

## Exerc√≠cios Pr√°ticos

### Exerc√≠cio 1: Hello World (Iniciante)

**Objetivo**: Criar sua primeira DAG.

```python
# dags/exercicio_01_hello.py
# Complete o c√≥digo:

from airflow.decorators import dag, task
from datetime import datetime

@dag(
    dag_id='exercicio_01_hello',
    schedule=None,  # Apenas trigger manual
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['exercicio'],
)
def exercicio_01():

    @task
    def hello():
        # TODO: Imprima "Hello, Airflow!"
        pass

    hello()

exercicio_01()
```

**Passos:**
1. Crie o arquivo em `dags/exercicio_01_hello.py`
2. Complete a fun√ß√£o `hello()`
3. Execute `astro dev start`
4. Acesse http://localhost:8080
5. Trigger a DAG manualmente
6. Verifique os logs

---

### Exerc√≠cio 2: Tasks em Sequ√™ncia (Iniciante)

**Objetivo**: Criar DAG com 3 tasks sequenciais.

```python
# dags/exercicio_02_sequencia.py
# Crie 3 tasks: extrair ‚Üí transformar ‚Üí carregar

from airflow.decorators import dag, task
from datetime import datetime

@dag(
    dag_id='exercicio_02_sequencia',
    schedule='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['exercicio'],
)
def exercicio_02():

    @task
    def extrair():
        # TODO: Retorne uma lista de n√∫meros [1, 2, 3, 4, 5]
        pass

    @task
    def transformar(dados):
        # TODO: Multiplique cada n√∫mero por 2
        pass

    @task
    def carregar(dados):
        # TODO: Imprima os dados transformados
        pass

    # TODO: Conecte as tasks

exercicio_02()
```

---

### Exerc√≠cio 3: XComs (Intermedi√°rio)

**Objetivo**: Passar dados entre tasks sem usar TaskFlow.

```python
# dags/exercicio_03_xcoms.py
# Use xcom_push e xcom_pull para passar dados

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def produtor(**context):
    dados = {'usuarios': 100, 'vendas': 5000}
    # TODO: Use context['ti'].xcom_push() para salvar dados
    pass

def consumidor(**context):
    # TODO: Use context['ti'].xcom_pull() para recuperar dados
    # TODO: Imprima os dados
    pass

with DAG(
    dag_id='exercicio_03_xcoms',
    schedule=None,
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['exercicio'],
) as dag:

    t1 = PythonOperator(
        task_id='produtor',
        python_callable=produtor,
    )

    t2 = PythonOperator(
        task_id='consumidor',
        python_callable=consumidor,
    )

    t1 >> t2
```

---

### Exerc√≠cio 4: Branching (Intermedi√°rio)

**Objetivo**: Criar fluxo condicional.

```python
# dags/exercicio_04_branching.py
# Crie uma DAG que:
# - Verifica se √© dia √∫til (segunda a sexta)
# - Se sim, executa "processar_relatorio"
# - Se n√£o, executa "pular_processamento"
# - Ambos convergem para "finalizar"

from airflow.decorators import dag, task
from datetime import datetime

@dag(
    dag_id='exercicio_04_branching',
    schedule='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['exercicio'],
)
def exercicio_04():

    @task.branch
    def verificar_dia(**context):
        # TODO: Verifique se execution_date √© dia √∫til
        # Retorne 'processar_relatorio' ou 'pular_processamento'
        pass

    @task
    def processar_relatorio():
        print("Processando relat√≥rio...")

    @task
    def pular_processamento():
        print("Fim de semana - pulando processamento")

    @task(trigger_rule='none_failed_min_one_success')
    def finalizar():
        print("Finalizando DAG")

    # TODO: Conecte as tasks

exercicio_04()
```

---

### Exerc√≠cio 5: Backfill (Avan√ßado)

**Objetivo**: Processar dados hist√≥ricos.

```python
# dags/exercicio_05_backfill.py
# Crie uma DAG que processa dados de uma data espec√≠fica

from airflow.decorators import dag, task
from datetime import datetime

@dag(
    dag_id='exercicio_05_backfill',
    schedule='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=True,  # Importante para backfill!
    tags=['exercicio'],
)
def exercicio_05():

    @task
    def processar_dia(ds=None, data_interval_start=None):
        """
        ds: string da data (YYYY-MM-DD)
        data_interval_start: datetime do in√≠cio do intervalo
        """
        print(f"Processando dados de: {ds}")
        # Simule processamento baseado na data
        return {'data': ds, 'registros': 100}

    processar_dia()

exercicio_05()
```

**Comandos para testar:**
```bash
# Backfill de janeiro/2024
astro dev run airflow dags backfill \
    -s 2024-01-01 \
    -e 2024-01-31 \
    exercicio_05_backfill

# Verificar execu√ß√µes
astro dev run airflow dags list-runs -d exercicio_05_backfill
```

---

## Gloss√°rio

| Termo | Defini√ß√£o |
|-------|-----------|
| **DAG** | Directed Acyclic Graph - defini√ß√£o de um workflow |
| **Task** | Unidade de trabalho dentro de uma DAG |
| **Operator** | Template de task (PythonOperator, BashOperator, etc.) |
| **Sensor** | Operator que aguarda uma condi√ß√£o |
| **XCom** | Mecanismo de comunica√ß√£o entre tasks |
| **execution_date** | Data l√≥gica de execu√ß√£o (deprecated, use logical_date) |
| **logical_date** | Data l√≥gica de execu√ß√£o (= data_interval_start) |
| **data_interval** | Per√≠odo de dados sendo processado |
| **Scheduler** | Componente que agenda e dispara execu√ß√µes |
| **Executor** | Componente que define como tasks s√£o executadas |
| **Worker** | Processo que executa as tasks |
| **Connection** | Credenciais de sistemas externos |
| **Variable** | Configura√ß√£o global acess√≠vel por todas DAGs |
| **Backfill** | Re-executar DAG para datas passadas |
| **Catchup** | Executar automaticamente datas pendentes ao ativar DAG |
| **TaskFlow API** | Forma moderna de escrever DAGs com decorators |
| **Dynamic Task Mapping** | Criar tasks dinamicamente baseado em dados |
| **Task Group** | Agrupamento visual de tasks |
| **Trigger Rule** | Regra que define quando uma task pode executar |

---

## Recursos Adicionais

- [Documenta√ß√£o Oficial Airflow](https://airflow.apache.org/docs/)
- [Astronomer Learn](https://www.astronomer.io/docs/learn) - Tutoriais excelentes
- [Awesome Apache Airflow](https://github.com/jghoman/awesome-apache-airflow)
- [Airflow Summit](https://airflowsummit.org/) - Confer√™ncia anual

---

‚Üí Voltar para [Roteiro de Onboarding](./roteiro-onboarding.md)

‚Üí Pr√≥ximo: [Setup Backend](./setup-backend.md)
