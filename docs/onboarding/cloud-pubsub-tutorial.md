# Cloud Pub/Sub: Filas de Mensagens na Nuvem

> Guia completo para entender e usar Google Cloud Pub/Sub. Do zero ao intermediÃ¡rio, com linguagem simples e exercÃ­cios prÃ¡ticos no contexto do DestaquesGovbr.

---

## Objetivos de Aprendizado

ApÃ³s completar este tutorial, vocÃª serÃ¡ capaz de:

- [ ] Entender o que sÃ£o sistemas de mensageria e filas
- [ ] Explicar quando usar Pub/Sub vs outras soluÃ§Ãµes
- [ ] Configurar tÃ³picos e subscriptions no GCP
- [ ] Publicar e consumir mensagens com Python
- [ ] Implementar retry e dead-letter queues
- [ ] Aplicar esses conceitos no data-platform

**Tempo estimado**: 4-6 horas

---

## Parte 1: Conceitos Fundamentais

### O Problema: ComunicaÃ§Ã£o SÃ­ncrona

Imagine o pipeline atual do scraper:

```mermaid
flowchart LR
    A[Scraper] -->|"1. Scrape"| B[NotÃ­cia]
    B -->|"2. Enrich"| C[Cogfy API]
    C -->|"3. Embed"| D[Embeddings API]
    D -->|"4. Save"| E[PostgreSQL]
    E -->|"5. Index"| F[Typesense]
```

**Problemas desse modelo:**

| Problema | ConsequÃªncia |
|----------|--------------|
| Falha em qualquer etapa | Todo o processo falha |
| API lenta | Scraper fica bloqueado esperando |
| Pico de carga | Sistema sobrecarrega |
| Retry manual | Precisa reprocessar tudo |

Isso Ã© chamado de **comunicaÃ§Ã£o sÃ­ncrona** - cada etapa espera a anterior terminar.

### A SoluÃ§Ã£o: Mensageria AssÃ­ncrona

Com mensageria, as etapas se comunicam atravÃ©s de **filas**:

```mermaid
flowchart LR
    A[Scraper] -->|publica| Q[(Fila)]
    Q -->|consome| B[Processador]
    B --> C[Cogfy]
    B --> D[Embeddings]
    B --> E[PostgreSQL]
    B --> F[Typesense]
```

**Vantagens:**

| Antes | Depois |
|-------|--------|
| Falha = erro total | Falha = mensagem fica na fila para retry |
| Bloqueio esperando | Scraper publica e segue em frente |
| Escalabilidade vertical | Escalabilidade horizontal |
| Acoplamento forte | Desacoplamento entre serviÃ§os |

### O que Ã© uma Fila de Mensagens?

Pense em uma **caixa de correio**:

1. VocÃª escreve uma carta (mensagem)
2. Coloca na caixa de correio (fila)
3. O carteiro (consumidor) pega quando puder
4. VocÃª nÃ£o precisa esperar o carteiro

```mermaid
flowchart LR
    subgraph "Produtor"
        A[AplicaÃ§Ã£o A]
    end

    subgraph "Fila"
        Q[ðŸ“¬ Mensagens]
    end

    subgraph "Consumidor"
        B[AplicaÃ§Ã£o B]
    end

    A -->|"envia"| Q
    Q -->|"recebe"| B
```

**CaracterÃ­sticas principais:**

- **AssÃ­ncrona**: produtor nÃ£o espera consumidor
- **Persistente**: mensagens sobrevivem a reinÃ­cios
- **Ordenada**: (geralmente) FIFO - primeiro a entrar, primeiro a sair
- **ConfiÃ¡vel**: mensagens nÃ£o se perdem

### PadrÃµes de Mensageria

Existem dois padrÃµes principais:

#### 1. Fila Ponto-a-Ponto (Queue)

Uma mensagem Ã© consumida por **um Ãºnico** consumidor:

```mermaid
flowchart LR
    P1[Produtor] --> Q[(Fila)]
    Q --> C1[Consumidor 1]
    Q --> C2[Consumidor 2]

    style C2 stroke-dasharray: 5 5
```

> Consumidor 1 OU Consumidor 2 recebe cada mensagem, nunca ambos.

**Exemplo**: Processar pedidos - cada pedido deve ser processado uma vez sÃ³.

#### 2. Pub/Sub (Publish-Subscribe)

Uma mensagem Ã© enviada para **mÃºltiplos** consumidores:

```mermaid
flowchart LR
    P[Publicador] --> T((TÃ³pico))
    T --> S1[Subscription 1]
    T --> S2[Subscription 2]
    S1 --> C1[Consumidor A]
    S2 --> C2[Consumidor B]
```

> Consumidor A E Consumidor B recebem a mesma mensagem.

**Exemplo**: NotificaÃ§Ã£o de nova notÃ­cia - sistema de busca E sistema de alertas recebem.

### Google Cloud Pub/Sub

O **Cloud Pub/Sub** Ã© o serviÃ§o de mensageria do Google Cloud. Ele implementa o padrÃ£o Pub/Sub (como o nome sugere).

```mermaid
flowchart TB
    subgraph "Publicadores"
        P1[Scraper 1]
        P2[Scraper 2]
    end

    subgraph "Pub/Sub"
        T((news-raw))
        S1[news-processor]
        S2[news-analytics]
    end

    subgraph "Consumidores"
        C1[Cloud Run - Processor]
        C2[BigQuery - Analytics]
    end

    P1 --> T
    P2 --> T
    T --> S1
    T --> S2
    S1 --> C1
    S2 --> C2
```

**Conceitos do Pub/Sub:**

| Conceito | DescriÃ§Ã£o | Analogia |
|----------|-----------|----------|
| **Topic** | Canal onde mensagens sÃ£o publicadas | RÃ¡dio FM |
| **Subscription** | "Assinatura" que recebe cÃ³pias das mensagens | Ouvinte sintonizado |
| **Message** | Dados enviados (atÃ© 10MB) | Programa de rÃ¡dio |
| **Publisher** | Quem envia mensagens ao tÃ³pico | DJ |
| **Subscriber** | Quem recebe mensagens da subscription | Ouvinte |

#### Fluxo de uma Mensagem

```mermaid
sequenceDiagram
    participant P as Publisher
    participant T as Topic
    participant S as Subscription
    participant C as Subscriber

    P->>T: 1. Publica mensagem
    T->>S: 2. Copia para subscriptions
    S->>C: 3. Entrega ao subscriber
    C->>S: 4. Acknowledge (ack)
    S->>S: 5. Remove mensagem
```

**Importante**: O subscriber deve enviar um **ack** (acknowledgment) para confirmar que processou a mensagem. Sem ack, a mensagem Ã© reenviada apÃ³s um tempo.

---

## Parte 2: MÃ£o na Massa com GCP

### PrÃ©-requisitos

Antes de comeÃ§ar, vocÃª precisa:

- [ ] Conta Google Cloud com acesso ao projeto `destaquesgovbr`
- [ ] `gcloud` CLI instalado ([instruÃ§Ãµes](https://cloud.google.com/sdk/docs/install))
- [ ] Python 3.11+ com Poetry
- [ ] Acesso ao repositÃ³rio `data-platform`

### Configurando o Ambiente

#### 1. Autenticar no GCP

```bash
# Login no GCP
gcloud auth login

# Configurar projeto
gcloud config set project destaquesgovbr

# Autenticar para bibliotecas Python
gcloud auth application-default login
```

#### 2. Instalar Bibliotecas Python

```bash
# No repositÃ³rio data-platform
poetry add google-cloud-pubsub
```

### Criando seu Primeiro TÃ³pico

Vamos criar um tÃ³pico de testes para aprender:

#### Via Console Web

1. Acesse [console.cloud.google.com/cloudpubsub](https://console.cloud.google.com/cloudpubsub)
2. Clique em "Create Topic"
3. Nome: `tutorial-test-topic`
4. Marque "Add a default subscription"
5. Clique em "Create"

#### Via CLI

```bash
# Criar tÃ³pico
gcloud pubsub topics create tutorial-test-topic

# Criar subscription
gcloud pubsub subscriptions create tutorial-test-sub \
    --topic=tutorial-test-topic \
    --ack-deadline=60
```

#### Via Terraform (produÃ§Ã£o)

```hcl
resource "google_pubsub_topic" "tutorial_test" {
  name = "tutorial-test-topic"
}

resource "google_pubsub_subscription" "tutorial_test_sub" {
  name  = "tutorial-test-sub"
  topic = google_pubsub_topic.tutorial_test.name

  ack_deadline_seconds = 60
}
```

### Publicando Mensagens

Crie um arquivo `tutorial_publisher.py`:

```python
"""
Tutorial: Publicando mensagens no Pub/Sub.

Execute: python tutorial_publisher.py
"""

import json
from google.cloud import pubsub_v1

# ConfiguraÃ§Ã£o
PROJECT_ID = "destaquesgovbr"
TOPIC_ID = "tutorial-test-topic"


def publish_message(data: dict) -> str:
    """
    Publica uma mensagem no tÃ³pico.

    Args:
        data: DicionÃ¡rio com os dados da mensagem

    Returns:
        ID da mensagem publicada
    """
    # Cria o cliente publisher
    publisher = pubsub_v1.PublisherClient()

    # Monta o caminho completo do tÃ³pico
    topic_path = publisher.topic_path(PROJECT_ID, TOPIC_ID)

    # Converte dados para bytes (obrigatÃ³rio)
    message_bytes = json.dumps(data).encode("utf-8")

    # Publica a mensagem
    # Atributos sÃ£o metadados opcionais (Ãºteis para filtros)
    future = publisher.publish(
        topic_path,
        message_bytes,
        source="tutorial",        # atributo customizado
        priority="normal",        # atributo customizado
    )

    # Aguarda confirmaÃ§Ã£o e retorna o ID
    message_id = future.result()
    print(f"âœ… Mensagem publicada: {message_id}")

    return message_id


def main():
    """Publica algumas mensagens de teste."""
    # Mensagem simples
    publish_message({
        "titulo": "Governo anuncia novo programa",
        "fonte": "planalto.gov.br",
        "timestamp": "2025-01-15T10:00:00Z",
    })

    # Outra mensagem
    publish_message({
        "titulo": "MinistÃ©rio divulga relatÃ³rio",
        "fonte": "gov.br/economia",
        "timestamp": "2025-01-15T11:00:00Z",
    })

    print("\nðŸ“¤ Todas as mensagens publicadas!")


if __name__ == "__main__":
    main()
```

Execute:

```bash
python tutorial_publisher.py
```

### Consumindo Mensagens

Existem dois modos de consumir mensagens:

#### Modo Pull (SÃ­ncrono)

O subscriber "puxa" mensagens quando quiser:

```python
"""
Tutorial: Consumindo mensagens (modo Pull).

Execute: python tutorial_subscriber_pull.py
"""

import json
from google.cloud import pubsub_v1

PROJECT_ID = "destaquesgovbr"
SUBSCRIPTION_ID = "tutorial-test-sub"


def pull_messages(max_messages: int = 10) -> list:
    """
    Puxa mensagens da subscription.

    Args:
        max_messages: NÃºmero mÃ¡ximo de mensagens a puxar

    Returns:
        Lista de mensagens processadas
    """
    subscriber = pubsub_v1.SubscriberClient()
    subscription_path = subscriber.subscription_path(PROJECT_ID, SUBSCRIPTION_ID)

    # Pull sÃ­ncrono
    response = subscriber.pull(
        request={
            "subscription": subscription_path,
            "max_messages": max_messages,
        }
    )

    messages = []
    ack_ids = []

    for received_message in response.received_messages:
        # Decodifica a mensagem
        data = json.loads(received_message.message.data.decode("utf-8"))
        print(f"ðŸ“¥ Recebido: {data}")

        messages.append(data)
        ack_ids.append(received_message.ack_id)

    # Confirma processamento (ack) de todas as mensagens
    if ack_ids:
        subscriber.acknowledge(
            request={
                "subscription": subscription_path,
                "ack_ids": ack_ids,
            }
        )
        print(f"\nâœ… {len(ack_ids)} mensagens confirmadas (ack)")

    return messages


if __name__ == "__main__":
    print("Puxando mensagens...\n")
    messages = pull_messages()

    if not messages:
        print("Nenhuma mensagem na fila.")
```

#### Modo Push (Streaming)

O subscriber recebe mensagens automaticamente (mais eficiente para alta carga):

```python
"""
Tutorial: Consumindo mensagens (modo Streaming).

Execute: python tutorial_subscriber_stream.py
Pressione Ctrl+C para parar.
"""

import json
import time
from concurrent.futures import TimeoutError
from google.cloud import pubsub_v1

PROJECT_ID = "destaquesgovbr"
SUBSCRIPTION_ID = "tutorial-test-sub"


def callback(message: pubsub_v1.subscriber.message.Message) -> None:
    """
    FunÃ§Ã£o chamada para cada mensagem recebida.

    Args:
        message: Mensagem do Pub/Sub
    """
    try:
        # Decodifica a mensagem
        data = json.loads(message.data.decode("utf-8"))

        # Acessa atributos (metadados)
        source = message.attributes.get("source", "unknown")
        priority = message.attributes.get("priority", "normal")

        print(f"ðŸ“¥ [{priority}] De {source}: {data}")

        # Simula processamento
        time.sleep(0.5)

        # Confirma que processou com sucesso
        message.ack()
        print("   âœ… Processado!")

    except Exception as e:
        print(f"   âŒ Erro: {e}")
        # Nack = mensagem serÃ¡ reenviada
        message.nack()


def main():
    """Inicia o subscriber em modo streaming."""
    subscriber = pubsub_v1.SubscriberClient()
    subscription_path = subscriber.subscription_path(PROJECT_ID, SUBSCRIPTION_ID)

    # Configura o subscriber
    flow_control = pubsub_v1.types.FlowControl(
        max_messages=10,           # MÃ¡ximo de mensagens simultÃ¢neas
        max_bytes=10 * 1024 * 1024  # 10MB
    )

    # Inicia a subscription
    streaming_pull_future = subscriber.subscribe(
        subscription_path,
        callback=callback,
        flow_control=flow_control,
    )

    print(f"ðŸŽ§ Escutando mensagens em {subscription_path}...")
    print("Pressione Ctrl+C para parar.\n")

    # MantÃ©m o processo rodando
    try:
        streaming_pull_future.result()
    except TimeoutError:
        streaming_pull_future.cancel()
        streaming_pull_future.result()
    except KeyboardInterrupt:
        streaming_pull_future.cancel()
        print("\nðŸ‘‹ Subscriber encerrado.")


if __name__ == "__main__":
    main()
```

### ExercÃ­cio 1: Publicar e Consumir

1. Em um terminal, rode o subscriber streaming:
   ```bash
   python tutorial_subscriber_stream.py
   ```

2. Em outro terminal, publique mensagens:
   ```bash
   python tutorial_publisher.py
   ```

3. Observe as mensagens chegando no subscriber.

---

## Parte 3: Recursos AvanÃ§ados

### Dead-Letter Queues

Mensagens que falham repetidamente sÃ£o enviadas para uma fila especial:

```mermaid
flowchart LR
    T((Topic)) --> S[Subscription]
    S -->|sucesso| C[Consumidor]
    S -->|"falha 5x"| DL[Dead-Letter Topic]
    DL --> DLS[DL Subscription]
    DLS --> A[AnÃ¡lise/Alerta]
```

#### Configurando Dead-Letter

```bash
# Criar tÃ³pico de dead-letter
gcloud pubsub topics create news-dead-letter

# Criar subscription com dead-letter
gcloud pubsub subscriptions create news-processor \
    --topic=news-raw \
    --dead-letter-topic=news-dead-letter \
    --max-delivery-attempts=5 \
    --ack-deadline=60
```

### Retry Policy

Configure quanto tempo esperar entre tentativas:

```bash
gcloud pubsub subscriptions update news-processor \
    --min-retry-delay=10s \
    --max-retry-delay=600s
```

Isso cria um **exponential backoff**:

| Tentativa | Delay |
|-----------|-------|
| 1Âª | 10s |
| 2Âª | 20s |
| 3Âª | 40s |
| 4Âª | 80s |
| 5Âª | 160s |
| 6Âª+ | 600s (mÃ¡ximo) |

### OrdenaÃ§Ã£o de Mensagens

Por padrÃ£o, Pub/Sub nÃ£o garante ordem. Para garantir, use **ordering keys**:

```python
# Publicar com ordering key
publisher.publish(
    topic_path,
    message_bytes,
    ordering_key="agency-planalto",  # Mensagens com mesma key sÃ£o ordenadas
)
```

### Filtragem de Mensagens

Crie subscriptions que recebem apenas mensagens especÃ­ficas:

```bash
# Subscription que sÃ³ recebe mensagens do Planalto
gcloud pubsub subscriptions create news-planalto-only \
    --topic=news-raw \
    --message-filter='attributes.agency="planalto"'
```

### Monitoramento

#### Via Console

- [Pub/Sub Monitoring](https://console.cloud.google.com/cloudpubsub/topic/list)
- MÃ©tricas: mensagens nÃ£o-acked, latÃªncia, throughput

#### MÃ©tricas Importantes

| MÃ©trica | O que significa | AÃ§Ã£o se alto |
|---------|-----------------|--------------|
| `unacked_messages` | Mensagens aguardando processamento | Escalar consumidores |
| `oldest_unacked_age` | Idade da mensagem mais antiga | Verificar se consumidor estÃ¡ travado |
| `dead_letter_messages` | Mensagens que falharam | Investigar erros |

---

## Parte 4: AplicaÃ§Ã£o no Data-Platform

### Arquitetura Proposta

```mermaid
flowchart TB
    subgraph "Scrapers"
        S1[Scraper Planalto]
        S2[Scraper MinSaude]
        S3[Scraper EBC]
    end

    subgraph "Pub/Sub"
        T((news-raw))
        SUB[news-processor-sub]
        DL((news-dead-letter))
    end

    subgraph "Cloud Run"
        P[News Processor Service]
    end

    subgraph "ServiÃ§os"
        C[Cogfy API]
        E[Embeddings API]
        PG[(PostgreSQL)]
        TS[(Typesense)]
    end

    S1 --> T
    S2 --> T
    S3 --> T
    T --> SUB
    SUB -->|"falha 5x"| DL
    SUB --> P
    P --> C
    P --> E
    P --> PG
    P --> TS
```

### Estrutura de Mensagem

```json
{
  "unique_id": "abc123def456",
  "agency": "planalto",
  "agency_name": "PalÃ¡cio do Planalto",
  "title": "Governo anuncia novo programa social",
  "url": "https://planalto.gov.br/noticias/...",
  "content": "O presidente anunciou hoje...",
  "published_at": "2025-01-15T10:00:00-03:00",
  "extracted_at": "2025-01-15T10:05:00-03:00",
  "image_url": "https://...",
  "category": "Economia",
  "tags": ["economia", "social"]
}
```

### Modificando o ScrapeManager

```python
# Antes (scrape_manager.py)
def _process_and_upload_data(self, new_data, allow_update: bool):
    new_data = self._preprocess_data(new_data)
    self.dataset_manager.insert(new_data, allow_update=allow_update)

# Depois
def _process_and_upload_data(self, new_data, allow_update: bool):
    new_data = self._preprocess_data(new_data)
    # Publica na fila em vez de inserir direto
    self.news_publisher.publish(new_data)
```

### NewsPublisher

```python
"""
Publisher de notÃ­cias para o Pub/Sub.

src/data_platform/queue/publisher.py
"""

import json
import logging
from typing import List, Dict, Any

from google.cloud import pubsub_v1

logger = logging.getLogger(__name__)


class NewsPublisher:
    """Publica notÃ­cias brutas para processamento assÃ­ncrono."""

    def __init__(
        self,
        project_id: str = "destaquesgovbr",
        topic_id: str = "news-raw",
    ):
        self.publisher = pubsub_v1.PublisherClient()
        self.topic_path = self.publisher.topic_path(project_id, topic_id)

    def publish(self, news_items: List[Dict[str, Any]]) -> List[str]:
        """
        Publica lista de notÃ­cias na fila.

        Args:
            news_items: Lista de dicionÃ¡rios com dados das notÃ­cias

        Returns:
            Lista de message IDs publicados
        """
        futures = []

        for item in news_items:
            data = json.dumps(item, ensure_ascii=False, default=str)
            message_bytes = data.encode("utf-8")

            future = self.publisher.publish(
                self.topic_path,
                message_bytes,
                agency=item.get("agency", "unknown"),
                source="scraper",
            )
            futures.append(future)

        # Aguarda confirmaÃ§Ã£o de todas
        message_ids = []
        for future in futures:
            try:
                message_id = future.result(timeout=30)
                message_ids.append(message_id)
            except Exception as e:
                logger.error(f"Erro publicando mensagem: {e}")

        logger.info(f"Publicadas {len(message_ids)}/{len(news_items)} mensagens")
        return message_ids

    def publish_single(self, news_item: Dict[str, Any]) -> str:
        """Publica uma Ãºnica notÃ­cia."""
        return self.publish([news_item])[0]
```

### NewsProcessor

```python
"""
Processador de notÃ­cias do Pub/Sub.

src/data_platform/queue/processor.py
"""

import json
import logging
from typing import Dict, Any

from google.cloud import pubsub_v1

from data_platform.managers.postgres_manager import PostgresManager
from data_platform.jobs.embeddings.embedding_generator import EmbeddingGenerator
from data_platform.typesense import get_client, index_documents

logger = logging.getLogger(__name__)


class NewsProcessor:
    """Processa notÃ­cias: enrichment â†’ embeddings â†’ PG â†’ Typesense."""

    def __init__(
        self,
        project_id: str = "destaquesgovbr",
        subscription_id: str = "news-raw-processor",
    ):
        self.subscriber = pubsub_v1.SubscriberClient()
        self.subscription_path = self.subscriber.subscription_path(
            project_id, subscription_id
        )

        # Inicializa serviÃ§os
        self.postgres = PostgresManager()
        self.embeddings = EmbeddingGenerator()
        self.typesense_client = get_client()

    def process_message(self, message: pubsub_v1.subscriber.message.Message) -> None:
        """
        Processa uma mensagem da fila.

        Args:
            message: Mensagem do Pub/Sub
        """
        unique_id = "unknown"
        try:
            # Decodifica a mensagem
            news_data = json.loads(message.data.decode("utf-8"))
            unique_id = news_data.get("unique_id", "unknown")

            logger.info(f"Processando: {unique_id}")

            # 1. Enrichment de tema (TODO: integrar Cogfy)
            # enriched = self.enrichment.enrich_single(news_data)
            enriched = news_data  # Por enquanto, sem enrichment

            # 2. Gera embedding
            embedding = self._generate_embedding(enriched)
            if embedding:
                enriched["content_embedding"] = embedding

            # 3. Insere no PostgreSQL
            self._save_to_postgres(enriched)

            # 4. Indexa no Typesense
            self._index_to_typesense(enriched)

            # Confirma processamento
            message.ack()
            logger.info(f"âœ… Processado: {unique_id}")

        except Exception as e:
            logger.error(f"âŒ Erro processando {unique_id}: {e}")
            # Nack = mensagem serÃ¡ reenviada
            message.nack()

    def _generate_embedding(self, news: Dict[str, Any]) -> list | None:
        """Gera embedding para a notÃ­cia."""
        try:
            text = self.embeddings._prepare_text_for_embedding(
                title=news.get("title", ""),
                summary=news.get("summary"),
                content=news.get("content"),
            )
            embeddings = self.embeddings._generate_embeddings_batch([text])
            return embeddings[0].tolist()
        except Exception as e:
            logger.warning(f"Erro gerando embedding: {e}")
            return None

    def _save_to_postgres(self, news: Dict[str, Any]) -> None:
        """Salva notÃ­cia no PostgreSQL."""
        from data_platform.models.news import NewsInsert

        news_insert = NewsInsert(
            unique_id=news["unique_id"],
            agency_key=news.get("agency"),
            title=news.get("title"),
            url=news.get("url"),
            content=news.get("content"),
            # ... outros campos
        )
        self.postgres.insert([news_insert], allow_update=True)

    def _index_to_typesense(self, news: Dict[str, Any]) -> None:
        """Indexa notÃ­cia no Typesense."""
        import pandas as pd
        from data_platform.typesense import index_documents

        df = pd.DataFrame([news])
        index_documents(self.typesense_client, df, mode="incremental", force=True)

    def start(self) -> None:
        """Inicia o processador em modo streaming."""
        logger.info(f"ðŸŽ§ Iniciando processador: {self.subscription_path}")

        streaming_pull_future = self.subscriber.subscribe(
            self.subscription_path,
            callback=self.process_message,
        )

        try:
            streaming_pull_future.result()
        except KeyboardInterrupt:
            streaming_pull_future.cancel()
            logger.info("Processador encerrado.")
        finally:
            self.postgres.close_all()
```

---

## Parte 5: ExercÃ­cios PrÃ¡ticos

### ExercÃ­cio 2: Criar TÃ³pico e Subscription

1. Crie um tÃ³pico chamado `{seu-nome}-test-topic`
2. Crie uma subscription para ele
3. Publique 5 mensagens
4. Consuma as mensagens e confirme o ack

### ExercÃ­cio 3: Simular Falhas

1. Modifique o subscriber para falhar (nack) em mensagens que contenham "erro"
2. Configure uma dead-letter queue
3. Publique mensagens com e sem "erro"
4. Verifique quais foram para a dead-letter

### ExercÃ­cio 4: Monitoramento

1. Acesse o console do Pub/Sub
2. Publique 100 mensagens sem consumidor ativo
3. Observe a mÃ©trica `unacked_messages` subir
4. Inicie o consumidor e observe a mÃ©trica descer

### ExercÃ­cio 5: IntegraÃ§Ã£o com Data-Platform

1. Clone o branch da issue #19
2. Rode os testes do NewsPublisher
3. Publique uma notÃ­cia de teste
4. Verifique se foi processada corretamente

---

## Limpeza

ApÃ³s os exercÃ­cios, limpe os recursos de teste:

```bash
# Deletar subscription
gcloud pubsub subscriptions delete tutorial-test-sub

# Deletar tÃ³pico
gcloud pubsub topics delete tutorial-test-topic
```

---

## Resumo de Comandos

| Comando | O que faz |
|---------|-----------|
| `gcloud pubsub topics create NOME` | Cria tÃ³pico |
| `gcloud pubsub topics list` | Lista tÃ³picos |
| `gcloud pubsub topics delete NOME` | Deleta tÃ³pico |
| `gcloud pubsub subscriptions create NOME --topic=TOPICO` | Cria subscription |
| `gcloud pubsub subscriptions list` | Lista subscriptions |
| `gcloud pubsub subscriptions pull SUB --limit=10` | Puxa mensagens (CLI) |

---

## Troubleshooting

### "Permission denied" ao publicar/consumir

```bash
# Verifique se estÃ¡ autenticado
gcloud auth list

# Reautentique
gcloud auth application-default login
```

### Mensagens nÃ£o chegam no subscriber

1. Verifique se a subscription estÃ¡ vinculada ao tÃ³pico correto
2. Verifique se nÃ£o hÃ¡ filtros configurados
3. Verifique a mÃ©trica `unacked_messages`

### Mensagens sendo reenviadas infinitamente

1. Verifique se o `message.ack()` estÃ¡ sendo chamado
2. Verifique se nÃ£o hÃ¡ exceÃ§Ãµes silenciosas
3. Configure uma dead-letter queue

---

## PrÃ³ximos Passos

ApÃ³s completar este tutorial:

1. **Implementar** a issue [#19 - Arquitetura de Filas](https://github.com/destaquesgovbr/data-platform/issues/19)
2. **Aprofundar** em Cloud Run + Pub/Sub
3. **Estudar** outros serviÃ§os GCP (Cloud Functions, Cloud Tasks)

---

## Recursos Adicionais

- [DocumentaÃ§Ã£o oficial do Cloud Pub/Sub](https://cloud.google.com/pubsub/docs)
- [Pub/Sub Python Client](https://cloud.google.com/python/docs/reference/pubsub/latest)
- [Patterns & Best Practices](https://cloud.google.com/pubsub/docs/subscriber)
- [Pricing Calculator](https://cloud.google.com/products/calculator)

---

> **Precisa de ajuda?** Consulte o [Troubleshooting](troubleshooting.md) ou pergunte no canal do time.
